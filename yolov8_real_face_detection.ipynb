{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://recruteur.lefigaro.fr/wp-content/uploads/2021/07/FR_yolo.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First You need to install ultralytics package \n",
    "```console\n",
    "!pip install ultralytics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data=\"config.yaml\", epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:01<00:00, 21.8MB/s]\n",
      "\n",
      "image 1/1 /home/emad/Documents/yolov8/images/test_image.jpg: 384x640 10 persons, 7 wine glasss, 7 cups, 6 bowls, 1 orange, 3 chairs, 1 potted plant, 1 dining table, 220.3ms\n",
      "Speed: 0.8ms preprocess, 220.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "#https://blog.paperspace.com/damaged-car-parts-detection-using-yolov8n-an-end-t/\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "img_pth = \"images/test_image.jpg\"\n",
    "model = YOLO(\"yolov8s.pt\") \n",
    "results = model(source=img_pth)\n",
    "res_plotted = results[0].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the output to jpg image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('images/test_image_output.jpg',res_plotted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow(\"result\", res_plotted)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.70 üöÄ Python-3.11.3 torch-2.0.0+cu117 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8n.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv8 requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /home/emad/miniconda3/envs/yolov8/lib/python3.11/site-packages (from onnx>=1.12.0) (1.24.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.20.2 in /home/emad/miniconda3/envs/yolov8/lib/python3.11/site-packages (from onnx>=1.12.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/emad/miniconda3/envs/yolov8/lib/python3.11/site-packages (from onnx>=1.12.0) (4.5.0)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.13.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 4.9s, saved as yolov8n.onnx (12.2 MB)\n",
      "\n",
      "Export complete (5.3s)\n",
      "Results saved to \u001b[1m/home/emad/Documents/yolov8\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "success = model.export(format=\"onnx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aad4d35444a1647a2b5e83f13c2ee74dbfc71222cb930938fbb1a7ba2f62af3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
